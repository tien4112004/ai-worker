import base64
import os
import random
from asyncio import sleep
from typing import Any, Dict, Generator, List, Tuple

from langchain_core.messages import HumanMessage, SystemMessage

from app.llms.executor import LLMExecutor
from app.prompts.loader import PromptStore
from app.schemas.image_content import ImageGenerateRequest
from app.schemas.mindmap_content import MindmapGenerateRequest
from app.schemas.slide_content import (
    OutlineGenerateRequest,
    PresentationGenerateRequest,
)
from app.schemas.token_usage import TokenUsage


class ContentService:
    def __init__(self, llm_executor: LLMExecutor, prompt_store: PromptStore):
        self.llm_executor = llm_executor or LLMExecutor()
        self.prompt_store = prompt_store or PromptStore()
        self.last_token_usage = None

    def _system(self, key: str, vars: Dict[str, Any] | None) -> str:
        return self.prompt_store.render(key, vars)

    # Presentation Generation
    def make_presentation_stream(self, request: PresentationGenerateRequest):
        """Generate slide content using LLM and save result.
        Args:
            request (PresentationGenerateRequest): Request object containing parameters for slide generation.
        Returns:
            Tuple: (chunks, token_usage) - list of content chunks and token usage data.
        """
        sys_msg = self._system(
            "presentation.system",
            None,
        )

        usr_msg = self._system(
            "presentation.user",
            request.to_dict(),
        )

        chunks, token_usage = self.llm_executor.stream(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Filter out token_usage objects (only check last chunk for efficiency)
        import json

        filtered_chunks = chunks[:-1] if chunks else []

        # Only parse the last chunk if it exists
        if chunks:
            last_chunk = chunks[-1]
            if not (
                last_chunk.startswith('{"token_usage"')
                or last_chunk.startswith('{"type":"token_usage"')
            ):
                filtered_chunks.append(last_chunk)

        # Store token usage for later access
        self.last_token_usage = token_usage
        return filtered_chunks, token_usage

    def make_presentation(self, request: PresentationGenerateRequest):
        """
        Generate slide content using LLM and save result.
        Args:
            request (PresentationGenerateRequest): Request object containing parameters for slide generation.
        Returns:
            Dict: A dictionary containing the generated slide content.
        """
        sys_msg = self._system(
            "presentation.system",
            None,
        )

        usr_msg = self._system(
            "presentation.user",
            request.to_dict(),
        )

        result, token_usage = self.llm_executor.batch(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    # Outline Generation
    def make_outline_stream(self, request: OutlineGenerateRequest):
        """Generate outline using LLM and save result.
        Args:
            request (OutlineGenerateRequest): Request object containing parameters for outline generation.
        Returns:
            Tuple: (chunks, token_usage) - list of content chunks and token usage data.
        """
        sys_msg = self._system(
            "outline.system",
            None,
        )

        usr_msg = self._system(
            "outline.user",
            request.to_dict(),
        )

        chunks, token_usage = self.llm_executor.stream(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return chunks, token_usage

    def make_outline(self, request: OutlineGenerateRequest):
        """Generate outline using LLM and save result.
        Args:
            request (OutlineGenerateRequest): Request object containing parameters for outline generation.
        Returns:
            Dict: A dictionary containing the generated outline.
        """
        sys_msg = self._system(
            "outline.system",
            None,
        )

        usr_msg = self._system(
            "outline.user",
            request.to_dict(),
        )

        result, token_usage = self.llm_executor.batch(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    def make_presentation_mock(
        self, request: PresentationGenerateRequest
    ) -> Tuple[str, TokenUsage]:
        """Generate mock slide content for testing purposes.
        Returns:
            Tuple: (result, token_usage) - mock slide content and zero token usage.
        """

        sys_msg = self._system(
            "presentation.system",
            request.to_dict(),
        )
        print("System Prompt:", sys_msg)  # Debug print

        result = '```json\n{\n  "slides": [\n    {\n      "type": "main_image",\n      "data": {\n        "image": "Children looking excitedly at an old map of Vietnam with a river highlighted",\n        "content": "Gi·ªõi thi·ªáu: M·ªôt Cu·ªôc Phi√™u L∆∞u L·ªãch S·ª≠ V·ªÅ S√¥ng B·∫°ch ƒê·∫±ng!"\n      }\n    },\n    {\n      "type": "two_column_with_image",\n      "title": "Ai ƒê√£ X√¢m L∆∞·ª£c N∆∞·ªõc Ta?",\n      "data": {\n        "items": [\n          "Qu√¢n ƒë·ªãch ƒë·∫øn t·ª´ n∆∞·ªõc Nam H√°n.",\n          "H·ªç mu·ªën chi·∫øm ƒë·∫•t n∆∞·ªõc ta.",\n          "Nh√¢n d√¢n ta kh√¥ng mu·ªën b·ªã m·∫•t n∆∞·ªõc."\n        ],\n        "image": "Illustration of ancient Chinese warships sailing towards Vietnamese shores"\n      }\n    },\n    {\n      "type": "two_column_with_image",\n      "title": "\\"B·∫´y\\" Tr√™n S√¥ng: √ù T∆∞·ªüng C·ªßa Ng√¥ Quy·ªÅn!",\n      "data": {\n        "items": [\n          "Ng√¥ Quy·ªÅn cho c·∫Øm c·ªçc nh·ªçn d∆∞·ªõi s√¥ng.",\n          "C·ªçc ·∫©n d∆∞·ªõi n∆∞·ªõc l√∫c tri·ªÅu l√™n.",\n          "Nh√¥ l√™n ƒë√¢m th·ªßng thuy·ªÅn ƒë·ªãch khi n∆∞·ªõc r√∫t."\n        ],\n        "image": "Illustration of a wooden stake hidden underwater in a river with a boat approaching"\n      }\n    },\n    {\n      "type": "two_column_with_image",\n      "title": "Tr·∫≠n Chi·∫øn R·ª±c L·ª≠a Tr√™n S√¥ng!",\n      "data": {\n        "items": [\n          "Thuy·ªÅn ƒë·ªãch m·∫Øc b·∫´y, b·ªã ƒë√¢m th·ªßng.",\n          "Qu√¢n ta t·∫•n c√¥ng t·ª´ hai b√™n b·ªù.",\n          "Chi·∫øn th·∫Øng vang d·ªôi cho d√¢n t·ªôc!"\n        ],\n        "image": "Illustration of Vietnamese soldiers attacking enemy ships from the riverbanks during a battle"\n      }\n    },\n    {\n      "type": "main_image",\n      "data": {\n        "image": "Illustration of a proud Vietnamese flag waving over a peaceful landscape",\n        "content": "Chi·∫øn th·∫Øng B·∫°ch ƒê·∫±ng gi√∫p ƒë·∫•t n∆∞·ªõc ta m√£i m√£i t·ª± do!"\n      }\n    }\n  ]\n}\n```'

        # Create zero token usage for mock
        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return result, mock_usage

    async def make_presentation_stream_mock(
        self,
    ) -> Tuple[List[Dict], TokenUsage]:
        """Generate mock presentation stream for testing purposes.
        Returns:
            Tuple: (slides, token_usage) - list of slide objects and zero token usage.
        """
        slides = [
            {
                "type": "main_image",
                "data": {
                    "image": "Children looking excitedly at an old map of Vietnam with a river highlighted",
                    "content": "Gi·ªõi thi·ªáu: M·ªôt Cu·ªôc Phi√™u L∆∞u L·ªãch S·ª≠ V·ªÅ S√¥ng B·∫°ch ƒê·∫±ng!",
                },
            },
            {
                "type": "two_column_with_image",
                "title": "Ai ƒê√£ X√¢m L∆∞·ª£c N∆∞·ªõc Ta?",
                "data": {
                    "items": [
                        "Qu√¢n ƒë·ªãch ƒë·∫øn t·ª´ n∆∞·ªõc Nam H√°n.",
                        "H·ªç mu·ªën chi·∫øm ƒë·∫•t n∆∞·ªõc ta.",
                        "Nh√¢n d√¢n ta kh√¥ng mu·ªën b·ªã m·∫•t n∆∞·ªõc.",
                    ],
                    "image": "Illustration of ancient Chinese warships sailing towards Vietnamese shores",
                },
            },
            {
                "type": "two_column_with_image",
                "title": '"B·∫´y" Tr√™n S√¥ng: √ù T∆∞·ªüng C·ªßa Ng√¥ Quy·ªÅn!',
                "data": {
                    "items": [
                        "Ng√¥ Quy·ªÅn cho c·∫Øm c·ªçc nh·ªçn d∆∞·ªõi s√¥ng.",
                        "C·ªçc ·∫©n d∆∞·ªõi n∆∞·ªõc l√∫c tri·ªÅu l√™n.",
                        "Nh√¥ l√™n ƒë√¢m th·ªßng thuy·ªÅn ƒë·ªãch khi n∆∞·ªõc r√∫t.",
                    ],
                    "image": "Illustration of a wooden stake hidden underwater in a river with a boat approaching",
                },
            },
            {
                "type": "two_column_with_image",
                "title": "Tr·∫≠n Chi·∫øn R·ª±c L·ª≠a Tr√™n S√¥ng!",
                "data": {
                    "items": [
                        "Thuy·ªÅn ƒë·ªãch m·∫Øc b·∫´y, b·ªã ƒë√¢m th·ªßng.",
                        "Qu√¢n ta t·∫•n c√¥ng t·ª´ hai b√™n b·ªù.",
                        "Chi·∫øn th·∫Øng vang d·ªôi cho d√¢n t·ªôc!",
                    ],
                    "image": "Illustration of Vietnamese soldiers attacking enemy ships from the riverbanks during a battle",
                },
            },
            {
                "type": "main_image",
                "data": {
                    "image": "Illustration of a proud Vietnamese flag waving over a peaceful landscape",
                    "content": "Chi·∫øn th·∫Øng B·∫°ch ƒê·∫±ng gi√∫p ƒë·∫•t n∆∞·ªõc ta m√£i m√£i t·ª± do!",
                },
            },
        ]

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return slides, mock_usage

    def make_outline_stream_mock(self) -> Tuple[List[str], TokenUsage]:
        """Generate mock outline stream for testing purposes.
        Returns:
            Tuple: (chunks, token_usage) - list of text chunks and zero token usage.
        """
        import re

        outline = '### Gi·ªõi thi·ªáu: M·ªôt Cu·ªôc Phi√™u L∆∞u L·ªãch S·ª≠ V·ªÅ S√¥ng B·∫°ch ƒê·∫±ng!\n\nCh√†o c√°c b·∫°n nh·ªè! H√¥m nay, ch√∫ng ta s·∫Ω c√πng nhau du h√†nh v·ªÅ qu√° kh·ª©, ƒë·∫øn v·ªõi m·ªôt kh√∫c s√¥ng th·∫≠t ƒë·∫∑c bi·ªát, n∆°i ƒë√£ di·ªÖn ra m·ªôt tr·∫≠n chi·∫øn l·ª´ng l·∫´y, gi√∫p b·∫£o v·ªá ƒë·∫•t n∆∞·ªõc Vi·ªát Nam c·ªßa ch√∫ng ta. C√°c b·∫°n ƒë√£ s·∫µn s√†ng ch∆∞a n√†o?\n\n*   Ch√∫ng ta s·∫Ω kh√°m ph√° c√¢u chuy·ªán v·ªÅ **S√¥ng B·∫°ch ƒê·∫±ng** ‚Äì m·ªôt d√≤ng s√¥ng h√πng vƒ©.\n*   T√¨m hi·ªÉu v·ªÅ nh·ªØng ng∆∞·ªùi anh h√πng d≈©ng c·∫£m ƒë√£ chi·∫øn ƒë·∫•u tr√™n d√≤ng s√¥ng n√†y.\n*   V√† hi·ªÉu t·∫°i sao tr·∫≠n chi·∫øn n√†y l·∫°i quan tr·ªçng ƒë·∫øn v·∫≠y!\n\n_H√£y chu·∫©n b·ªã tinh th·∫ßn ƒë·ªÉ tr·ªü th√†nh nh·ªØng nh√† th√°m hi·ªÉm l·ªãch s·ª≠ nh√©!_\n\n### Ai ƒê√£ X√¢m L∆∞·ª£c N∆∞·ªõc Ta?\n\nNg√†y x∆∞a, c√≥ nh·ªØng ƒë·ªôi qu√¢n t·ª´ ph∆∞∆°ng B·∫Øc mu·ªën x√¢m chi·∫øm ƒë·∫•t n∆∞·ªõc ta. H·ªç r·∫•t ƒë√¥ng v√† m·∫°nh m·∫Ω, gi·ªëng nh∆∞ m·ªôt c∆°n b√£o s·∫Øp ·∫≠p ƒë·∫øn v·∫≠y.\n\n*   Qu√¢n ƒë·ªãch ƒë·∫øn t·ª´ **n∆∞·ªõc Nam H√°n** (nay thu·ªôc Trung Qu·ªëc).\n*   H·ªç mu·ªën chi·∫øm ƒë√≥ng v√† cai tr·ªã ƒë·∫•t n∆∞·ªõc c·ªßa ch√∫ng ta.\n*   Nh√¢n d√¢n ta r·∫•t lo s·ª£, nh∆∞ng kh√¥ng h·ªÅ mu·ªën b·ªã m·∫•t n∆∞·ªõc.\n\n> T∆∞·ªüng t∆∞·ª£ng xem, n·∫øu c√≥ ai ƒë√≥ mu·ªën l·∫•y ƒëi ƒë·ªì ch∆°i y√™u th√≠ch c·ªßa b·∫°n, b·∫°n s·∫Ω l√†m g√¨? √îng cha ta c≈©ng ƒë√£ r·∫•t quy·∫øt t√¢m b·∫£o v·ªá ƒë·∫•t n∆∞·ªõc m√¨nh!\n\n### "B·∫´y" Tr√™n S√¥ng: √ù T∆∞·ªüng Tuy·ªát V·ªùi C·ªßa Ng√¥ Quy·ªÅn!\n\nƒê·ªÉ ch·ªëng l·∫°i qu√¢n ƒë·ªãch m·∫°nh m·∫Ω, Ng√¥ Quy·ªÅn ‚Äì v·ªã t∆∞·ªõng t√†i ba c·ªßa ch√∫ng ta ‚Äì ƒë√£ nghƒ© ra m·ªôt k·∫ø ho·∫°ch v√¥ c√πng th√¥ng minh v√† ƒë·ªôc ƒë√°o. ƒê√≥ l√† s·ª≠ d·ª•ng ch√≠nh d√≤ng s√¥ng B·∫°ch ƒê·∫±ng ƒë·ªÉ l√†m "chi·∫øn tr∆∞·ªùng"!\n\n*   Ng√¥ Quy·ªÅn cho qu√¢n l√≠nh **c·∫Øm c·ªçc nh·ªçn** xu·ªëng l√≤ng s√¥ng, ·∫©n d∆∞·ªõi m·∫∑t n∆∞·ªõc l√∫c tri·ªÅu l√™n.\n*   Khi **tri·ªÅu r√∫t**, nh·ªØng chi·∫øc c·ªçc n√†y s·∫Ω nh√¥ l√™n, s·∫µn s√†ng ƒë√¢m th·ªßng thuy·ªÅn ƒë·ªãch.\n*   ƒê√¢y l√† m·ªôt c√°i b·∫´y thi√™n nhi√™n tuy·ªát v·ªùi!\n\n_Gi·ªëng nh∆∞ ch√∫ng ta giƒÉng b·∫´y chu·ªôt v·∫≠y ƒë√≥, nh∆∞ng l√† b·∫´y cho thuy·ªÅn l·ªõn!_\n\n### Tr·∫≠n Chi·∫øn R·ª±c L·ª≠a Tr√™n S√¥ng!\n\nKhi qu√¢n Nam H√°n h√πng h·ªï ti·∫øn v√†o s√¥ng B·∫°ch ƒê·∫±ng, h·ªç ƒë√£ m·∫Øc b·∫´y c·ªßa Ng√¥ Quy·ªÅn.\n\n*   Thuy·ªÅn ƒë·ªãch b·ªã **ƒë√¢m th·ªßng** b·ªüi nh·ªØng chi·∫øc c·ªçc nh·ªçn khi n∆∞·ªõc r√∫t.\n*   Qu√¢n ta t·ª´ hai b√™n b·ªù s√¥ng ƒë√£ **t·∫•n c√¥ng d·ªØ d·ªôi**.\n*   Tr·∫≠n chi·∫øn di·ªÖn ra v√¥ c√πng √°c li·ªát, nh∆∞ng qu√¢n ta ƒë√£ chi·∫øn th·∫Øng vang d·ªôi!\n\n> Ti·∫øng reo h√≤ vang v·ªçng kh·∫Øp s√¥ng, ƒë√°nh d·∫•u m·ªôt chi·∫øn th·∫Øng v·∫ª vang cho d√¢n t·ªôc!\n\n### √ù Nghƒ©a L·ªãch S·ª≠: V√¨ Sao Ch√∫ng Ta Nh·ªõ M√£i?\n\nChi·∫øn th·∫Øng s√¥ng B·∫°ch ƒê·∫±ng kh√¥ng ch·ªâ l√† m·ªôt tr·∫≠n ƒë√°nh hay, m√† n√≥ c√≤n mang m·ªôt √Ω nghƒ©a v√¥ c√πng to l·ªõn ƒë·ªëi v·ªõi l·ªãch s·ª≠ Vi·ªát Nam.\n\n*   Tr·∫≠n chi·∫øn n√†y ƒë√£ gi√∫p **gi·∫£i ph√≥ng ƒë·∫•t n∆∞·ªõc** kh·ªèi √°ch ƒë√¥ h·ªô c·ªßa qu√¢n Nam H√°n.\n*   N√≥ kh·∫≥ng ƒë·ªãnh √Ω ch√≠ **quy·∫øt t√¢m gi·ªØ g√¨n non s√¥ng** c·ªßa d√¢n t·ªôc ta.\n*   Ng√¥ Quy·ªÅn tr·ªü th√†nh v·ªã vua, m·ªü ra m·ªôt th·ªùi k·ª≥ ƒë·ªôc l·∫≠p m·ªõi cho ƒë·∫•t n∆∞·ªõc.\n\n_Nh·ªù c√≥ nh·ªØng ng∆∞·ªùi anh h√πng nh∆∞ Ng√¥ Quy·ªÅn v√† chi·∫øn th·∫Øng B·∫°ch ƒê·∫±ng, Vi·ªát Nam ch√∫ng ta m·ªõi ƒë∆∞·ª£c t·ª± do v√† ph√°t tri·ªÉn cho ƒë·∫øn ng√†y nay!_'
        # Split the outline into meaningful chunks (words and punctuation)
        chunks = re.findall(r"\S+|\s+", outline)

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return chunks, mock_usage

    def make_outline_mock(
        self, outlineGenerateRequest: OutlineGenerateRequest
    ) -> Tuple[str, TokenUsage]:
        """Generate mock outline for testing purposes.
        Returns:
            Tuple: (outline, token_usage) - mock outline content and zero token usage.
        """

        sys_msg = self._system(
            "outline.system",
            outlineGenerateRequest.to_dict(),
        )
        print("System Prompt:", sys_msg)  # Debug print

        usr_sys_msg = self._system(
            "outline.user",
            outlineGenerateRequest.to_dict(),
        )
        print("User Prompt:", usr_sys_msg)  # Debug print

        outline = '### Gi·ªõi thi·ªáu: M·ªôt Cu·ªôc Phi√™u L∆∞u L·ªãch S·ª≠ V·ªÅ S√¥ng B·∫°ch ƒê·∫±ng!\n\nCh√†o c√°c b·∫°n nh·ªè! H√¥m nay, ch√∫ng ta s·∫Ω c√πng nhau du h√†nh v·ªÅ qu√° kh·ª©, ƒë·∫øn v·ªõi m·ªôt kh√∫c s√¥ng th·∫≠t ƒë·∫∑c bi·ªát, n∆°i ƒë√£ di·ªÖn ra m·ªôt tr·∫≠n chi·∫øn l·ª´ng l·∫´y, gi√∫p b·∫£o v·ªá ƒë·∫•t n∆∞·ªõc Vi·ªát Nam c·ªßa ch√∫ng ta. C√°c b·∫°n ƒë√£ s·∫µn s√†ng ch∆∞a n√†o?\n\n*   Ch√∫ng ta s·∫Ω kh√°m ph√° c√¢u chuy·ªán v·ªÅ **S√¥ng B·∫°ch ƒê·∫±ng** ‚Äì m·ªôt d√≤ng s√¥ng h√πng vƒ©.\n*   T√¨m hi·ªÉu v·ªÅ nh·ªØng ng∆∞·ªùi anh h√πng d≈©ng c·∫£m ƒë√£ chi·∫øn ƒë·∫•u tr√™n d√≤ng s√¥ng n√†y.\n*   V√† hi·ªÉu t·∫°i sao tr·∫≠n chi·∫øn n√†y l·∫°i quan tr·ªçng ƒë·∫øn v·∫≠y!\n\n_H√£y chu·∫©n b·ªã tinh th·∫ßn ƒë·ªÉ tr·ªü th√†nh nh·ªØng nh√† th√°m hi·ªÉm l·ªãch s·ª≠ nh√©!_\n\n### Ai ƒê√£ X√¢m L∆∞·ª£c N∆∞·ªõc Ta?\n\nNg√†y x∆∞a, c√≥ nh·ªØng ƒë·ªôi qu√¢n t·ª´ ph∆∞∆°ng B·∫Øc mu·ªën x√¢m chi·∫øm ƒë·∫•t n∆∞·ªõc ta. H·ªç r·∫•t ƒë√¥ng v√† m·∫°nh m·∫Ω, gi·ªëng nh∆∞ m·ªôt c∆°n b√£o s·∫Øp ·∫≠p ƒë·∫øn v·∫≠y.\n\n*   Qu√¢n ƒë·ªãch ƒë·∫øn t·ª´ **n∆∞·ªõc Nam H√°n** (nay thu·ªôc Trung Qu·ªëc).\n*   H·ªç mu·ªën chi·∫øm ƒë√≥ng v√† cai tr·ªã ƒë·∫•t n∆∞·ªõc c·ªßa ch√∫ng ta.\n*   Nh√¢n d√¢n ta r·∫•t lo s·ª£, nh∆∞ng kh√¥ng h·ªÅ mu·ªën b·ªã m·∫•t n∆∞·ªõc.\n\n> T∆∞·ªüng t∆∞·ª£ng xem, n·∫øu c√≥ ai ƒë√≥ mu·ªën l·∫•y ƒëi ƒë·ªì ch∆°i y√™u th√≠ch c·ªßa b·∫°n, b·∫°n s·∫Ω l√†m g√¨? √îng cha ta c≈©ng ƒë√£ r·∫•t quy·∫øt t√¢m b·∫£o v·ªá ƒë·∫•t n∆∞·ªõc m√¨nh!\n\n### "B·∫´y" Tr√™n S√¥ng: √ù T∆∞·ªüng Tuy·ªát V·ªùi C·ªßa Ng√¥ Quy·ªÅn!\n\nƒê·ªÉ ch·ªëng l·∫°i qu√¢n ƒë·ªãch m·∫°nh m·∫Ω, Ng√¥ Quy·ªÅn ‚Äì v·ªã t∆∞·ªõng t√†i ba c·ªßa ch√∫ng ta ‚Äì ƒë√£ nghƒ© ra m·ªôt k·∫ø ho·∫°ch v√¥ c√πng th√¥ng minh v√† ƒë·ªôc ƒë√°o. ƒê√≥ l√† s·ª≠ d·ª•ng ch√≠nh d√≤ng s√¥ng B·∫°ch ƒê·∫±ng ƒë·ªÉ l√†m "chi·∫øn tr∆∞·ªùng"!\n\n*   Ng√¥ Quy·ªÅn cho qu√¢n l√≠nh **c·∫Øm c·ªçc nh·ªçn** xu·ªëng l√≤ng s√¥ng, ·∫©n d∆∞·ªõi m·∫∑t n∆∞·ªõc l√∫c tri·ªÅu l√™n.\n*   Khi **tri·ªÅu r√∫t**, nh·ªØng chi·∫øc c·ªçc n√†y s·∫Ω nh√¥ l√™n, s·∫µn s√†ng ƒë√¢m th·ªßng thuy·ªÅn ƒë·ªãch.\n*   ƒê√¢y l√† m·ªôt c√°i b·∫´y thi√™n nhi√™n tuy·ªát v·ªùi!\n\n_Gi·ªëng nh∆∞ ch√∫ng ta giƒÉng b·∫´y chu·ªôt v·∫≠y ƒë√≥, nh∆∞ng l√† b·∫´y cho thuy·ªÅn l·ªõn!_\n\n### Tr·∫≠n Chi·∫øn R·ª±c L·ª≠a Tr√™n S√¥ng!\n\nKhi qu√¢n Nam H√°n h√πng h·ªï ti·∫øn v√†o s√¥ng B·∫°ch ƒê·∫±ng, h·ªç ƒë√£ m·∫Øc b·∫´y c·ªßa Ng√¥ Quy·ªÅn.\n\n*   Thuy·ªÅn ƒë·ªãch b·ªã **ƒë√¢m th·ªßng** b·ªüi nh·ªØng chi·∫øc c·ªçc nh·ªçn khi n∆∞·ªõc r√∫t.\n*   Qu√¢n ta t·ª´ hai b√™n b·ªù s√¥ng ƒë√£ **t·∫•n c√¥ng d·ªØ d·ªôi**.\n*   Tr·∫≠n chi·∫øn di·ªÖn ra v√¥ c√πng √°c li·ªát, nh∆∞ng qu√¢n ta ƒë√£ chi·∫øn th·∫Øng vang d·ªôi!\n\n> Ti·∫øng reo h√≤ vang v·ªçng kh·∫Øp s√¥ng, ƒë√°nh d·∫•u m·ªôt chi·∫øn th·∫Øng v·∫ª vang cho d√¢n t·ªôc!\n\n### √ù Nghƒ©a L·ªãch S·ª≠: V√¨ Sao Ch√∫ng Ta Nh·ªõ M√£i?\n\nChi·∫øn th·∫Øng s√¥ng B·∫°ch ƒê·∫±ng kh√¥ng ch·ªâ l√† m·ªôt tr·∫≠n ƒë√°nh hay, m√† n√≥ c√≤n mang m·ªôt √Ω nghƒ©a v√¥ c√πng to l·ªõn ƒë·ªëi v·ªõi l·ªãch s·ª≠ Vi·ªát Nam.\n\n*   Tr·∫≠n chi·∫øn n√†y ƒë√£ gi√∫p **gi·∫£i ph√≥ng ƒë·∫•t n∆∞·ªõc** kh·ªèi √°ch ƒë√¥ h·ªô c·ªßa qu√¢n Nam H√°n.\n*   N√≥ kh·∫≥ng ƒë·ªãnh √Ω ch√≠ **quy·∫øt t√¢m gi·ªØ g√¨n non s√¥ng** c·ªßa d√¢n t·ªôc ta.\n*   Ng√¥ Quy·ªÅn tr·ªü th√†nh v·ªã vua, m·ªü ra m·ªôt th·ªùi k·ª≥ ƒë·ªôc l·∫≠p m·ªõi cho ƒë·∫•t n∆∞·ªõc.\n\n_Nh·ªù c√≥ nh·ªØng ng∆∞·ªùi anh h√πng nh∆∞ Ng√¥ Quy·ªÅn v√† chi·∫øn th·∫Øng B·∫°ch ƒê·∫±ng, Vi·ªát Nam ch√∫ng ta m·ªõi ƒë∆∞·ª£c t·ª± do v√† ph√°t tri·ªÉn cho ƒë·∫øn ng√†y nay!_'

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return outline, mock_usage

    def generate_image(self, request: ImageGenerateRequest):
        """Generate image based on text description"""

        usr_msg = self._system("image.user", {"prompt": request.prompt})

        result = self.llm_executor.generate_image(
            provider=request.provider,
            model=request.model,
            message=usr_msg,
            number_of_images=request.number_of_images,
            aspect_ratio=request.aspect_ratio,
            safety_filter_level=request.safety_filter_level,
            person_generation=request.person_generation,
            seed=request.seed,
            negative_prompt=request.negative_prompt,
        )
        return result

    def generate_image_mock(self, request: ImageGenerateRequest):
        """Generate mock image data for testing purposes."""
        sleep(random.uniform(0.3, 1.5))  # Simulate some processing delay
        with open("app/services/image_mock.png", "rb") as f:
            mock_image_data = base64.b64encode(f.read()).decode("utf-8")

        images = [mock_image_data for _ in range(request.number_of_images)]
        return {
            "images": images,
            "count": request.number_of_images,
            "error": None,
        }

    def generate_mindmap(self, request: MindmapGenerateRequest):
        sys_msg = self._system(
            "mindmap.system",
            request.to_dict(),
        )

        usr_msg = self._system(
            "mindmap.user",
            request.to_dict(),
        )

        result, token_usage = self.llm_executor.batch(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    def generate_mindmap_mock(
        self, request: MindmapGenerateRequest
    ) -> Tuple[str, TokenUsage]:
        """Generate mock mindmap data for testing purposes.
        Returns:
            Tuple: (mindmap, token_usage) - mock mindmap content and zero token usage.
        """
        sleep(random.uniform(1.4, 1.5))  # Simulate some processing delay

        mock_mindmap = """
        ```json
    {
        "content": "Th·∫ø gi·ªõi xung quanh em",
        "children": [
            {
            "content": "ƒê·ªông v·∫≠t üêæ",
            "children": [
                {
                "content": "ƒê·ªông v·∫≠t c√≥ v√∫",
                "children": [
                    { "content": "Ch√≥ - b·∫°n th√¢n c·ªßa con ng∆∞·ªùi" },
                    { "content": "M√®o - lo√†i v·∫≠t tinh ngh·ªãch" },
                    { "content": "Voi - lo√†i v·∫≠t to l·ªõn" }
                ]
                },
                {
                "content": "Chim üê¶",
                "children": [
                    { "content": "Chim s·∫ª - h√≥t l√≠u lo m·ªói s√°ng" },
                    { "content": "Chim c√°nh c·ª•t - s·ªëng ·ªü x·ª© l·∫°nh" },
                    { "content": "ƒê·∫°i b√†ng - ch√∫a t·ªÉ b·∫ßu tr·ªùi" }
                ]
                },
                {
                "content": "C√¥n tr√πng üêû",
                "children": [
                    { "content": "Ong - chƒÉm ch·ªâ l√†m m·∫≠t" },
                    { "content": "B∆∞·ªõm - xinh ƒë·∫πp v·ªõi ƒë√¥i c√°nh" }
                ]
                }
            ]
            },
            {
            "content": "Th·ª±c v·∫≠t üå±",
            "children": [
                {
                "content": "C√¢y xanh",
                "children": [
                    { "content": "C√¢y ƒÉn qu·∫£ - cho ta tr√°i ngon" },
                    { "content": "C√¢y b√≥ng m√°t - che r·ª£p ƒë∆∞·ªùng ƒëi" }
                ]
                },
                {
                "content": "Hoa üå∏",
                "children": [
                    { "content": "Hoa h·ªìng - bi·ªÉu t∆∞·ª£ng c·ªßa t√¨nh y√™u" },
                    { "content": "Hoa h∆∞·ªõng d∆∞∆°ng - lu√¥n h∆∞·ªõng v·ªÅ m·∫∑t tr·ªùi" }
                ]
                },
                {
                "content": "Rau c·ªß ü•ï",
                "children": [
                    { "content": "C√† r·ªët - t·ªët cho m·∫Øt" },
                    { "content": "B·∫Øp c·∫£i - nhi·ªÅu vitamin" }
                ]
                }
            ]
            },
            {
            "content": "Thi√™n nhi√™n üèûÔ∏è",
            "children": [
                {
                "content": "N√∫i non h√πng vƒ©",
                "children": [
                    { "content": "ƒê·ªânh n√∫i cao v√∫t" },
                    { "content": "Thung l≈©ng xanh m∆∞·ªõt" }
                ]
                },
                {
                "content": "Bi·ªÉn c·∫£ bao la üåä",
                "children": [
                    { "content": "S√≥ng v·ªó r√¨ r√†o" },
                    { "content": "Sinh v·∫≠t bi·ªÉn ƒëa d·∫°ng" }
                ]
                },
                {
                "content": "Th·ªùi ti·∫øt ‚òÄÔ∏èüåßÔ∏è",
                "children": [
                    { "content": "Tr·ªùi n·∫Øng - ·∫•m √°p" },
                    { "content": "Tr·ªùi m∆∞a - m√°t m·∫ª" },
                    { "content": "Tr·ªùi gi√≥ - th·ªïi m·∫°nh" }
                ]
                }
            ]
            }
        ]
    }
        ```
        """

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return mock_mindmap, mock_usage

    # ============ RAG ============
    def make_outline_with_rag(self, request: OutlineGenerateRequest):
        """Generate outline using LLM and save result.
        Args:
            request (OutlineGenerateRequest): Request object containing parameters for outline generation.
        Returns:
            Dict: A dictionary containing the generated outline.
        """
        sys_msg = self._system(
            "outline.system.rag",
            None,
        )

        usr_msg = self._system(
            "outline.user",
            request.to_dict(),
        )

        # Build filters for document search
        # Note: Use subject_code (e.g., 'TV', 'T', 'TA') instead of subject name
        filters = {}
        if request.subject:
            filters["subject_code"] = request.subject
        if request.grade:
            # Convert grade to integer if it's numeric (metadata stores it as int)
            try:
                filters["grade"] = int(request.grade)
            except (ValueError, TypeError):
                filters["grade"] = request.grade

        print(f"[DEBUG] RAG filters being applied: {filters}")
        print(
            f"[DEBUG] Request - subject: {request.subject}, grade: {request.grade} (type: {type(request.grade).__name__})"
        )

        result, token_usage = self.llm_executor.rag_batch(
            provider=request.provider,
            model=request.model,
            query=usr_msg,
            system_prompt=sys_msg,
            return_source_documents=True,
            filters=filters if filters else None,
            custom_prompt=None,
            verbose=False,
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    def make_presentation_with_rag(self, request: PresentationGenerateRequest):
        sys_msg = self._system(
            "presentation.system.rag",
            None,
        )

        usr_msg = self._system(
            "presentation.user",
            request.to_dict(),
        )

        filters = {}
        if request.subject:
            filters["subject_code"] = request.subject
        if request.grade:
            try:
                filters["grade"] = int(request.grade)
            except (ValueError, TypeError):
                filters["grade"] = request.grade

        print(f"[DEBUG] RAG filters being applied: {filters}")
        print(
            f"[DEBUG] Request - subject: {request.subject}, grade: {request.grade} (type: {type(request.grade).__name__})"
        )

        result, token_usage = self.llm_executor.rag_batch(
            provider=request.provider,
            model=request.model,
            query=usr_msg,
            system_prompt=sys_msg,
            return_source_documents=True,
            filters=filters if filters else None,
            custom_prompt=None,
            verbose=False,
        )

        self.last_token_usage = token_usage
        return result

    def generate_mindmap_with_rag(self, request: MindmapGenerateRequest):
        sys_msg = self._system(
            "mindmap.system.rag",
            None,
        )

        usr_msg = self._system(
            "mindmap.user",
            request.to_dict(),
        )

        filters = {}
        if request.subject:
            filters["subject_code"] = request.subject
        if request.grade:
            try:
                filters["grade"] = int(request.grade)
            except (ValueError, TypeError):
                filters["grade"] = request.grade

        print(f"[DEBUG] RAG filters being applied: {filters}")
        print(
            f"[DEBUG] Request - subject: {request.subject}, grade: {request.grade} (type: {type(request.grade).__name__})"
        )

        result, token_usage = self.llm_executor.rag_batch(
            provider=request.provider,
            model=request.model,
            query=usr_msg,
            system_prompt=sys_msg,
            return_source_documents=True,
            filters=filters if filters else None,
            custom_prompt=None,
            verbose=False,
        )

        self.last_token_usage = token_usage
        return result
