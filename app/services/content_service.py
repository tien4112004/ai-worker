import base64
import os
import random
from asyncio import sleep
from typing import Any, Dict, Generator, List, Tuple

from langchain_core.messages import HumanMessage, SystemMessage

from app.llms.executor import LLMExecutor
from app.prompts.loader import PromptStore
from app.schemas.image_content import ImageGenerateRequest
from app.schemas.mindmap_content import MindmapGenerateRequest
from app.schemas.slide_content import (
    OutlineGenerateRequest,
    PresentationGenerateRequest,
)
from app.schemas.token_usage import TokenUsage


class ContentService:
    def __init__(self, llm_executor: LLMExecutor, prompt_store: PromptStore):
        self.llm_executor = llm_executor or LLMExecutor()
        self.prompt_store = prompt_store or PromptStore()
        self.last_token_usage = None

    def _system(self, key: str, vars: Dict[str, Any] | None) -> str:
        return self.prompt_store.render(key, vars)

    # Presentation Generation
    def make_presentation_stream(self, request: PresentationGenerateRequest):
        """Generate slide content using LLM and save result.
        Args:
            request (PresentationGenerateRequest): Request object containing parameters for slide generation.
        Returns:
            Tuple: (chunks, token_usage) - list of content chunks and token usage data.
        """
        sys_msg = self._system(
            "presentation.system",
            None,
        )

        usr_msg = self._system(
            "presentation.user",
            request.to_dict(),
        )

        chunks, token_usage = self.llm_executor.stream(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Filter out token_usage objects (only check last chunk for efficiency)
        import json

        filtered_chunks = chunks[:-1] if chunks else []

        # Only parse the last chunk if it exists
        if chunks:
            last_chunk = chunks[-1]
            if not (
                last_chunk.startswith('{"token_usage"')
                or last_chunk.startswith('{"type":"token_usage"')
            ):
                filtered_chunks.append(last_chunk)

        # Store token usage for later access
        self.last_token_usage = token_usage
        return filtered_chunks, token_usage

    def make_presentation(self, request: PresentationGenerateRequest):
        """
        Generate slide content using LLM and save result.
        Args:
            request (PresentationGenerateRequest): Request object containing parameters for slide generation.
        Returns:
            Dict: A dictionary containing the generated slide content.
        """
        sys_msg = self._system(
            "presentation.system",
            None,
        )

        usr_msg = self._system(
            "presentation.user",
            request.to_dict(),
        )

        result, token_usage = self.llm_executor.batch(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    # Outline Generation
    def make_outline_stream(self, request: OutlineGenerateRequest):
        """Generate outline using LLM and save result.
        Args:
            request (OutlineGenerateRequest): Request object containing parameters for outline generation.
        Returns:
            Tuple: (chunks, token_usage) - list of content chunks and token usage data.
        """
        sys_msg = self._system(
            "outline.system",
            None,
        )

        usr_msg = self._system(
            "outline.user",
            request.to_dict(),
        )

        chunks, token_usage = self.llm_executor.stream(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return chunks, token_usage

    def make_outline(self, request: OutlineGenerateRequest):
        """Generate outline using LLM and save result.
        Args:
            request (OutlineGenerateRequest): Request object containing parameters for outline generation.
        Returns:
            Dict: A dictionary containing the generated outline.
        """
        sys_msg = self._system(
            "outline.system",
            None,
        )

        usr_msg = self._system(
            "outline.user",
            request.to_dict(),
        )

        result, token_usage = self.llm_executor.batch(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    def make_presentation_mock(
        self, request: PresentationGenerateRequest
    ) -> Tuple[str, TokenUsage]:
        """Generate mock slide content for testing purposes.
        Returns:
            Tuple: (result, token_usage) - mock slide content and zero token usage.
        """

        sys_msg = self._system(
            "presentation.system",
            request.to_dict(),
        )
        print("System Prompt:", sys_msg)  # Debug print

        result = '```json\n{\n  "slides": [\n    {\n      "type": "main_image",\n      "data": {\n        "image": "Children looking excitedly at an old map of Vietnam with a river highlighted",\n        "content": "Giá»›i thiá»‡u: Má»™t Cuá»™c PhiÃªu LÆ°u Lá»‹ch Sá»­ Vá» SÃ´ng Báº¡ch Äáº±ng!"\n      }\n    },\n    {\n      "type": "two_column_with_image",\n      "title": "Ai ÄÃ£ XÃ¢m LÆ°á»£c NÆ°á»›c Ta?",\n      "data": {\n        "items": [\n          "QuÃ¢n Ä‘á»‹ch Ä‘áº¿n tá»« nÆ°á»›c Nam HÃ¡n.",\n          "Há» muá»‘n chiáº¿m Ä‘áº¥t nÆ°á»›c ta.",\n          "NhÃ¢n dÃ¢n ta khÃ´ng muá»‘n bá»‹ máº¥t nÆ°á»›c."\n        ],\n        "image": "Illustration of ancient Chinese warships sailing towards Vietnamese shores"\n      }\n    },\n    {\n      "type": "two_column_with_image",\n      "title": "\\"Báº«y\\" TrÃªn SÃ´ng: Ã TÆ°á»Ÿng Cá»§a NgÃ´ Quyá»n!",\n      "data": {\n        "items": [\n          "NgÃ´ Quyá»n cho cáº¯m cá»c nhá»n dÆ°á»›i sÃ´ng.",\n          "Cá»c áº©n dÆ°á»›i nÆ°á»›c lÃºc triá»u lÃªn.",\n          "NhÃ´ lÃªn Ä‘Ã¢m thá»§ng thuyá»n Ä‘á»‹ch khi nÆ°á»›c rÃºt."\n        ],\n        "image": "Illustration of a wooden stake hidden underwater in a river with a boat approaching"\n      }\n    },\n    {\n      "type": "two_column_with_image",\n      "title": "Tráº­n Chiáº¿n Rá»±c Lá»­a TrÃªn SÃ´ng!",\n      "data": {\n        "items": [\n          "Thuyá»n Ä‘á»‹ch máº¯c báº«y, bá»‹ Ä‘Ã¢m thá»§ng.",\n          "QuÃ¢n ta táº¥n cÃ´ng tá»« hai bÃªn bá».",\n          "Chiáº¿n tháº¯ng vang dá»™i cho dÃ¢n tá»™c!"\n        ],\n        "image": "Illustration of Vietnamese soldiers attacking enemy ships from the riverbanks during a battle"\n      }\n    },\n    {\n      "type": "main_image",\n      "data": {\n        "image": "Illustration of a proud Vietnamese flag waving over a peaceful landscape",\n        "content": "Chiáº¿n tháº¯ng Báº¡ch Äáº±ng giÃºp Ä‘áº¥t nÆ°á»›c ta mÃ£i mÃ£i tá»± do!"\n      }\n    }\n  ]\n}\n```'

        # Create zero token usage for mock
        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return result, mock_usage

    async def make_presentation_stream_mock(
        self,
    ) -> Tuple[List[Dict], TokenUsage]:
        """Generate mock presentation stream for testing purposes.
        Returns:
            Tuple: (slides, token_usage) - list of slide objects and zero token usage.
        """
        slides = [
            {
                "type": "main_image",
                "data": {
                    "image": "Children looking excitedly at an old map of Vietnam with a river highlighted",
                    "content": "Giá»›i thiá»‡u: Má»™t Cuá»™c PhiÃªu LÆ°u Lá»‹ch Sá»­ Vá» SÃ´ng Báº¡ch Äáº±ng!",
                },
            },
            {
                "type": "two_column_with_image",
                "title": "Ai ÄÃ£ XÃ¢m LÆ°á»£c NÆ°á»›c Ta?",
                "data": {
                    "items": [
                        "QuÃ¢n Ä‘á»‹ch Ä‘áº¿n tá»« nÆ°á»›c Nam HÃ¡n.",
                        "Há» muá»‘n chiáº¿m Ä‘áº¥t nÆ°á»›c ta.",
                        "NhÃ¢n dÃ¢n ta khÃ´ng muá»‘n bá»‹ máº¥t nÆ°á»›c.",
                    ],
                    "image": "Illustration of ancient Chinese warships sailing towards Vietnamese shores",
                },
            },
            {
                "type": "two_column_with_image",
                "title": '"Báº«y" TrÃªn SÃ´ng: Ã TÆ°á»Ÿng Cá»§a NgÃ´ Quyá»n!',
                "data": {
                    "items": [
                        "NgÃ´ Quyá»n cho cáº¯m cá»c nhá»n dÆ°á»›i sÃ´ng.",
                        "Cá»c áº©n dÆ°á»›i nÆ°á»›c lÃºc triá»u lÃªn.",
                        "NhÃ´ lÃªn Ä‘Ã¢m thá»§ng thuyá»n Ä‘á»‹ch khi nÆ°á»›c rÃºt.",
                    ],
                    "image": "Illustration of a wooden stake hidden underwater in a river with a boat approaching",
                },
            },
            {
                "type": "two_column_with_image",
                "title": "Tráº­n Chiáº¿n Rá»±c Lá»­a TrÃªn SÃ´ng!",
                "data": {
                    "items": [
                        "Thuyá»n Ä‘á»‹ch máº¯c báº«y, bá»‹ Ä‘Ã¢m thá»§ng.",
                        "QuÃ¢n ta táº¥n cÃ´ng tá»« hai bÃªn bá».",
                        "Chiáº¿n tháº¯ng vang dá»™i cho dÃ¢n tá»™c!",
                    ],
                    "image": "Illustration of Vietnamese soldiers attacking enemy ships from the riverbanks during a battle",
                },
            },
            {
                "type": "main_image",
                "data": {
                    "image": "Illustration of a proud Vietnamese flag waving over a peaceful landscape",
                    "content": "Chiáº¿n tháº¯ng Báº¡ch Äáº±ng giÃºp Ä‘áº¥t nÆ°á»›c ta mÃ£i mÃ£i tá»± do!",
                },
            },
        ]

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return slides, mock_usage

    def make_outline_stream_mock(self) -> Tuple[List[str], TokenUsage]:
        """Generate mock outline stream for testing purposes.
        Returns:
            Tuple: (chunks, token_usage) - list of text chunks and zero token usage.
        """
        import re

        outline = '### Giá»›i thiá»‡u: Má»™t Cuá»™c PhiÃªu LÆ°u Lá»‹ch Sá»­ Vá» SÃ´ng Báº¡ch Äáº±ng!\n\nChÃ o cÃ¡c báº¡n nhá»! HÃ´m nay, chÃºng ta sáº½ cÃ¹ng nhau du hÃ nh vá» quÃ¡ khá»©, Ä‘áº¿n vá»›i má»™t khÃºc sÃ´ng tháº­t Ä‘áº·c biá»‡t, nÆ¡i Ä‘Ã£ diá»…n ra má»™t tráº­n chiáº¿n lá»«ng láº«y, giÃºp báº£o vá»‡ Ä‘áº¥t nÆ°á»›c Viá»‡t Nam cá»§a chÃºng ta. CÃ¡c báº¡n Ä‘Ã£ sáºµn sÃ ng chÆ°a nÃ o?\n\n*   ChÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¢u chuyá»‡n vá» **SÃ´ng Báº¡ch Äáº±ng** â€“ má»™t dÃ²ng sÃ´ng hÃ¹ng vÄ©.\n*   TÃ¬m hiá»ƒu vá» nhá»¯ng ngÆ°á»i anh hÃ¹ng dÅ©ng cáº£m Ä‘Ã£ chiáº¿n Ä‘áº¥u trÃªn dÃ²ng sÃ´ng nÃ y.\n*   VÃ  hiá»ƒu táº¡i sao tráº­n chiáº¿n nÃ y láº¡i quan trá»ng Ä‘áº¿n váº­y!\n\n_HÃ£y chuáº©n bá»‹ tinh tháº§n Ä‘á»ƒ trá»Ÿ thÃ nh nhá»¯ng nhÃ  thÃ¡m hiá»ƒm lá»‹ch sá»­ nhÃ©!_\n\n### Ai ÄÃ£ XÃ¢m LÆ°á»£c NÆ°á»›c Ta?\n\nNgÃ y xÆ°a, cÃ³ nhá»¯ng Ä‘á»™i quÃ¢n tá»« phÆ°Æ¡ng Báº¯c muá»‘n xÃ¢m chiáº¿m Ä‘áº¥t nÆ°á»›c ta. Há» ráº¥t Ä‘Ã´ng vÃ  máº¡nh máº½, giá»‘ng nhÆ° má»™t cÆ¡n bÃ£o sáº¯p áº­p Ä‘áº¿n váº­y.\n\n*   QuÃ¢n Ä‘á»‹ch Ä‘áº¿n tá»« **nÆ°á»›c Nam HÃ¡n** (nay thuá»™c Trung Quá»‘c).\n*   Há» muá»‘n chiáº¿m Ä‘Ã³ng vÃ  cai trá»‹ Ä‘áº¥t nÆ°á»›c cá»§a chÃºng ta.\n*   NhÃ¢n dÃ¢n ta ráº¥t lo sá»£, nhÆ°ng khÃ´ng há» muá»‘n bá»‹ máº¥t nÆ°á»›c.\n\n> TÆ°á»Ÿng tÆ°á»£ng xem, náº¿u cÃ³ ai Ä‘Ã³ muá»‘n láº¥y Ä‘i Ä‘á»“ chÆ¡i yÃªu thÃ­ch cá»§a báº¡n, báº¡n sáº½ lÃ m gÃ¬? Ã”ng cha ta cÅ©ng Ä‘Ã£ ráº¥t quyáº¿t tÃ¢m báº£o vá»‡ Ä‘áº¥t nÆ°á»›c mÃ¬nh!\n\n### "Báº«y" TrÃªn SÃ´ng: Ã TÆ°á»Ÿng Tuyá»‡t Vá»i Cá»§a NgÃ´ Quyá»n!\n\nÄá»ƒ chá»‘ng láº¡i quÃ¢n Ä‘á»‹ch máº¡nh máº½, NgÃ´ Quyá»n â€“ vá»‹ tÆ°á»›ng tÃ i ba cá»§a chÃºng ta â€“ Ä‘Ã£ nghÄ© ra má»™t káº¿ hoáº¡ch vÃ´ cÃ¹ng thÃ´ng minh vÃ  Ä‘á»™c Ä‘Ã¡o. ÄÃ³ lÃ  sá»­ dá»¥ng chÃ­nh dÃ²ng sÃ´ng Báº¡ch Äáº±ng Ä‘á»ƒ lÃ m "chiáº¿n trÆ°á»ng"!\n\n*   NgÃ´ Quyá»n cho quÃ¢n lÃ­nh **cáº¯m cá»c nhá»n** xuá»‘ng lÃ²ng sÃ´ng, áº©n dÆ°á»›i máº·t nÆ°á»›c lÃºc triá»u lÃªn.\n*   Khi **triá»u rÃºt**, nhá»¯ng chiáº¿c cá»c nÃ y sáº½ nhÃ´ lÃªn, sáºµn sÃ ng Ä‘Ã¢m thá»§ng thuyá»n Ä‘á»‹ch.\n*   ÄÃ¢y lÃ  má»™t cÃ¡i báº«y thiÃªn nhiÃªn tuyá»‡t vá»i!\n\n_Giá»‘ng nhÆ° chÃºng ta giÄƒng báº«y chuá»™t váº­y Ä‘Ã³, nhÆ°ng lÃ  báº«y cho thuyá»n lá»›n!_\n\n### Tráº­n Chiáº¿n Rá»±c Lá»­a TrÃªn SÃ´ng!\n\nKhi quÃ¢n Nam HÃ¡n hÃ¹ng há»• tiáº¿n vÃ o sÃ´ng Báº¡ch Äáº±ng, há» Ä‘Ã£ máº¯c báº«y cá»§a NgÃ´ Quyá»n.\n\n*   Thuyá»n Ä‘á»‹ch bá»‹ **Ä‘Ã¢m thá»§ng** bá»Ÿi nhá»¯ng chiáº¿c cá»c nhá»n khi nÆ°á»›c rÃºt.\n*   QuÃ¢n ta tá»« hai bÃªn bá» sÃ´ng Ä‘Ã£ **táº¥n cÃ´ng dá»¯ dá»™i**.\n*   Tráº­n chiáº¿n diá»…n ra vÃ´ cÃ¹ng Ã¡c liá»‡t, nhÆ°ng quÃ¢n ta Ä‘Ã£ chiáº¿n tháº¯ng vang dá»™i!\n\n> Tiáº¿ng reo hÃ² vang vá»ng kháº¯p sÃ´ng, Ä‘Ã¡nh dáº¥u má»™t chiáº¿n tháº¯ng váº» vang cho dÃ¢n tá»™c!\n\n### Ã NghÄ©a Lá»‹ch Sá»­: VÃ¬ Sao ChÃºng Ta Nhá»› MÃ£i?\n\nChiáº¿n tháº¯ng sÃ´ng Báº¡ch Äáº±ng khÃ´ng chá»‰ lÃ  má»™t tráº­n Ä‘Ã¡nh hay, mÃ  nÃ³ cÃ²n mang má»™t Ã½ nghÄ©a vÃ´ cÃ¹ng to lá»›n Ä‘á»‘i vá»›i lá»‹ch sá»­ Viá»‡t Nam.\n\n*   Tráº­n chiáº¿n nÃ y Ä‘Ã£ giÃºp **giáº£i phÃ³ng Ä‘áº¥t nÆ°á»›c** khá»i Ã¡ch Ä‘Ã´ há»™ cá»§a quÃ¢n Nam HÃ¡n.\n*   NÃ³ kháº³ng Ä‘á»‹nh Ã½ chÃ­ **quyáº¿t tÃ¢m giá»¯ gÃ¬n non sÃ´ng** cá»§a dÃ¢n tá»™c ta.\n*   NgÃ´ Quyá»n trá»Ÿ thÃ nh vá»‹ vua, má»Ÿ ra má»™t thá»i ká»³ Ä‘á»™c láº­p má»›i cho Ä‘áº¥t nÆ°á»›c.\n\n_Nhá» cÃ³ nhá»¯ng ngÆ°á»i anh hÃ¹ng nhÆ° NgÃ´ Quyá»n vÃ  chiáº¿n tháº¯ng Báº¡ch Äáº±ng, Viá»‡t Nam chÃºng ta má»›i Ä‘Æ°á»£c tá»± do vÃ  phÃ¡t triá»ƒn cho Ä‘áº¿n ngÃ y nay!_'
        # Split the outline into meaningful chunks (words and punctuation)
        chunks = re.findall(r"\S+|\s+", outline)

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return chunks, mock_usage

    def make_outline_mock(
        self, outlineGenerateRequest: OutlineGenerateRequest
    ) -> Tuple[str, TokenUsage]:
        """Generate mock outline for testing purposes.
        Returns:
            Tuple: (outline, token_usage) - mock outline content and zero token usage.
        """

        sys_msg = self._system(
            "outline.system",
            outlineGenerateRequest.to_dict(),
        )
        print("System Prompt:", sys_msg)  # Debug print

        usr_sys_msg = self._system(
            "outline.user",
            outlineGenerateRequest.to_dict(),
        )
        print("User Prompt:", usr_sys_msg)  # Debug print

        outline = '### Giá»›i thiá»‡u: Má»™t Cuá»™c PhiÃªu LÆ°u Lá»‹ch Sá»­ Vá» SÃ´ng Báº¡ch Äáº±ng!\n\nChÃ o cÃ¡c báº¡n nhá»! HÃ´m nay, chÃºng ta sáº½ cÃ¹ng nhau du hÃ nh vá» quÃ¡ khá»©, Ä‘áº¿n vá»›i má»™t khÃºc sÃ´ng tháº­t Ä‘áº·c biá»‡t, nÆ¡i Ä‘Ã£ diá»…n ra má»™t tráº­n chiáº¿n lá»«ng láº«y, giÃºp báº£o vá»‡ Ä‘áº¥t nÆ°á»›c Viá»‡t Nam cá»§a chÃºng ta. CÃ¡c báº¡n Ä‘Ã£ sáºµn sÃ ng chÆ°a nÃ o?\n\n*   ChÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¢u chuyá»‡n vá» **SÃ´ng Báº¡ch Äáº±ng** â€“ má»™t dÃ²ng sÃ´ng hÃ¹ng vÄ©.\n*   TÃ¬m hiá»ƒu vá» nhá»¯ng ngÆ°á»i anh hÃ¹ng dÅ©ng cáº£m Ä‘Ã£ chiáº¿n Ä‘áº¥u trÃªn dÃ²ng sÃ´ng nÃ y.\n*   VÃ  hiá»ƒu táº¡i sao tráº­n chiáº¿n nÃ y láº¡i quan trá»ng Ä‘áº¿n váº­y!\n\n_HÃ£y chuáº©n bá»‹ tinh tháº§n Ä‘á»ƒ trá»Ÿ thÃ nh nhá»¯ng nhÃ  thÃ¡m hiá»ƒm lá»‹ch sá»­ nhÃ©!_\n\n### Ai ÄÃ£ XÃ¢m LÆ°á»£c NÆ°á»›c Ta?\n\nNgÃ y xÆ°a, cÃ³ nhá»¯ng Ä‘á»™i quÃ¢n tá»« phÆ°Æ¡ng Báº¯c muá»‘n xÃ¢m chiáº¿m Ä‘áº¥t nÆ°á»›c ta. Há» ráº¥t Ä‘Ã´ng vÃ  máº¡nh máº½, giá»‘ng nhÆ° má»™t cÆ¡n bÃ£o sáº¯p áº­p Ä‘áº¿n váº­y.\n\n*   QuÃ¢n Ä‘á»‹ch Ä‘áº¿n tá»« **nÆ°á»›c Nam HÃ¡n** (nay thuá»™c Trung Quá»‘c).\n*   Há» muá»‘n chiáº¿m Ä‘Ã³ng vÃ  cai trá»‹ Ä‘áº¥t nÆ°á»›c cá»§a chÃºng ta.\n*   NhÃ¢n dÃ¢n ta ráº¥t lo sá»£, nhÆ°ng khÃ´ng há» muá»‘n bá»‹ máº¥t nÆ°á»›c.\n\n> TÆ°á»Ÿng tÆ°á»£ng xem, náº¿u cÃ³ ai Ä‘Ã³ muá»‘n láº¥y Ä‘i Ä‘á»“ chÆ¡i yÃªu thÃ­ch cá»§a báº¡n, báº¡n sáº½ lÃ m gÃ¬? Ã”ng cha ta cÅ©ng Ä‘Ã£ ráº¥t quyáº¿t tÃ¢m báº£o vá»‡ Ä‘áº¥t nÆ°á»›c mÃ¬nh!\n\n### "Báº«y" TrÃªn SÃ´ng: Ã TÆ°á»Ÿng Tuyá»‡t Vá»i Cá»§a NgÃ´ Quyá»n!\n\nÄá»ƒ chá»‘ng láº¡i quÃ¢n Ä‘á»‹ch máº¡nh máº½, NgÃ´ Quyá»n â€“ vá»‹ tÆ°á»›ng tÃ i ba cá»§a chÃºng ta â€“ Ä‘Ã£ nghÄ© ra má»™t káº¿ hoáº¡ch vÃ´ cÃ¹ng thÃ´ng minh vÃ  Ä‘á»™c Ä‘Ã¡o. ÄÃ³ lÃ  sá»­ dá»¥ng chÃ­nh dÃ²ng sÃ´ng Báº¡ch Äáº±ng Ä‘á»ƒ lÃ m "chiáº¿n trÆ°á»ng"!\n\n*   NgÃ´ Quyá»n cho quÃ¢n lÃ­nh **cáº¯m cá»c nhá»n** xuá»‘ng lÃ²ng sÃ´ng, áº©n dÆ°á»›i máº·t nÆ°á»›c lÃºc triá»u lÃªn.\n*   Khi **triá»u rÃºt**, nhá»¯ng chiáº¿c cá»c nÃ y sáº½ nhÃ´ lÃªn, sáºµn sÃ ng Ä‘Ã¢m thá»§ng thuyá»n Ä‘á»‹ch.\n*   ÄÃ¢y lÃ  má»™t cÃ¡i báº«y thiÃªn nhiÃªn tuyá»‡t vá»i!\n\n_Giá»‘ng nhÆ° chÃºng ta giÄƒng báº«y chuá»™t váº­y Ä‘Ã³, nhÆ°ng lÃ  báº«y cho thuyá»n lá»›n!_\n\n### Tráº­n Chiáº¿n Rá»±c Lá»­a TrÃªn SÃ´ng!\n\nKhi quÃ¢n Nam HÃ¡n hÃ¹ng há»• tiáº¿n vÃ o sÃ´ng Báº¡ch Äáº±ng, há» Ä‘Ã£ máº¯c báº«y cá»§a NgÃ´ Quyá»n.\n\n*   Thuyá»n Ä‘á»‹ch bá»‹ **Ä‘Ã¢m thá»§ng** bá»Ÿi nhá»¯ng chiáº¿c cá»c nhá»n khi nÆ°á»›c rÃºt.\n*   QuÃ¢n ta tá»« hai bÃªn bá» sÃ´ng Ä‘Ã£ **táº¥n cÃ´ng dá»¯ dá»™i**.\n*   Tráº­n chiáº¿n diá»…n ra vÃ´ cÃ¹ng Ã¡c liá»‡t, nhÆ°ng quÃ¢n ta Ä‘Ã£ chiáº¿n tháº¯ng vang dá»™i!\n\n> Tiáº¿ng reo hÃ² vang vá»ng kháº¯p sÃ´ng, Ä‘Ã¡nh dáº¥u má»™t chiáº¿n tháº¯ng váº» vang cho dÃ¢n tá»™c!\n\n### Ã NghÄ©a Lá»‹ch Sá»­: VÃ¬ Sao ChÃºng Ta Nhá»› MÃ£i?\n\nChiáº¿n tháº¯ng sÃ´ng Báº¡ch Äáº±ng khÃ´ng chá»‰ lÃ  má»™t tráº­n Ä‘Ã¡nh hay, mÃ  nÃ³ cÃ²n mang má»™t Ã½ nghÄ©a vÃ´ cÃ¹ng to lá»›n Ä‘á»‘i vá»›i lá»‹ch sá»­ Viá»‡t Nam.\n\n*   Tráº­n chiáº¿n nÃ y Ä‘Ã£ giÃºp **giáº£i phÃ³ng Ä‘áº¥t nÆ°á»›c** khá»i Ã¡ch Ä‘Ã´ há»™ cá»§a quÃ¢n Nam HÃ¡n.\n*   NÃ³ kháº³ng Ä‘á»‹nh Ã½ chÃ­ **quyáº¿t tÃ¢m giá»¯ gÃ¬n non sÃ´ng** cá»§a dÃ¢n tá»™c ta.\n*   NgÃ´ Quyá»n trá»Ÿ thÃ nh vá»‹ vua, má»Ÿ ra má»™t thá»i ká»³ Ä‘á»™c láº­p má»›i cho Ä‘áº¥t nÆ°á»›c.\n\n_Nhá» cÃ³ nhá»¯ng ngÆ°á»i anh hÃ¹ng nhÆ° NgÃ´ Quyá»n vÃ  chiáº¿n tháº¯ng Báº¡ch Äáº±ng, Viá»‡t Nam chÃºng ta má»›i Ä‘Æ°á»£c tá»± do vÃ  phÃ¡t triá»ƒn cho Ä‘áº¿n ngÃ y nay!_'

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return outline, mock_usage

    def generate_image(self, request: ImageGenerateRequest):
        """Generate image based on text description"""

        usr_msg = self._system("image.user", {"prompt": request.prompt})

        result = self.llm_executor.generate_image(
            provider=request.provider,
            model=request.model,
            message=usr_msg,
            number_of_images=request.number_of_images,
            aspect_ratio=request.aspect_ratio,
            safety_filter_level=request.safety_filter_level,
            person_generation=request.person_generation,
            seed=request.seed,
            negative_prompt=request.negative_prompt,
        )
        return result

    def generate_image_mock(self, request: ImageGenerateRequest):
        """Generate mock image data for testing purposes."""
        sleep(random.uniform(0.3, 1.5))  # Simulate some processing delay
        with open("app/services/image_mock.png", "rb") as f:
            mock_image_data = base64.b64encode(f.read()).decode("utf-8")

        images = [mock_image_data for _ in range(request.number_of_images)]
        return {
            "images": images,
            "count": request.number_of_images,
            "error": None,
        }

    def generate_mindmap(self, request: MindmapGenerateRequest):
        sys_msg = self._system(
            "mindmap.system",
            request.to_dict(),
        )

        usr_msg = self._system(
            "mindmap.user",
            request.to_dict(),
        )

        result, token_usage = self.llm_executor.batch(
            provider=request.provider,
            model=request.model,
            messages=[
                SystemMessage(content=sys_msg),
                HumanMessage(content=usr_msg),
            ],
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    def generate_mindmap_mock(
        self, request: MindmapGenerateRequest
    ) -> Tuple[str, TokenUsage]:
        """Generate mock mindmap data for testing purposes.
        Returns:
            Tuple: (mindmap, token_usage) - mock mindmap content and zero token usage.
        """
        sleep(random.uniform(1.4, 1.5))  # Simulate some processing delay

        mock_mindmap = """
        ```json
    {
        "content": "Tháº¿ giá»›i xung quanh em",
        "children": [
            {
            "content": "Äá»™ng váº­t ğŸ¾",
            "children": [
                {
                "content": "Äá»™ng váº­t cÃ³ vÃº",
                "children": [
                    { "content": "ChÃ³ - báº¡n thÃ¢n cá»§a con ngÆ°á»i" },
                    { "content": "MÃ¨o - loÃ i váº­t tinh nghá»‹ch" },
                    { "content": "Voi - loÃ i váº­t to lá»›n" }
                ]
                },
                {
                "content": "Chim ğŸ¦",
                "children": [
                    { "content": "Chim sáº» - hÃ³t lÃ­u lo má»—i sÃ¡ng" },
                    { "content": "Chim cÃ¡nh cá»¥t - sá»‘ng á»Ÿ xá»© láº¡nh" },
                    { "content": "Äáº¡i bÃ ng - chÃºa tá»ƒ báº§u trá»i" }
                ]
                },
                {
                "content": "CÃ´n trÃ¹ng ğŸ",
                "children": [
                    { "content": "Ong - chÄƒm chá»‰ lÃ m máº­t" },
                    { "content": "BÆ°á»›m - xinh Ä‘áº¹p vá»›i Ä‘Ã´i cÃ¡nh" }
                ]
                }
            ]
            },
            {
            "content": "Thá»±c váº­t ğŸŒ±",
            "children": [
                {
                "content": "CÃ¢y xanh",
                "children": [
                    { "content": "CÃ¢y Äƒn quáº£ - cho ta trÃ¡i ngon" },
                    { "content": "CÃ¢y bÃ³ng mÃ¡t - che rá»£p Ä‘Æ°á»ng Ä‘i" }
                ]
                },
                {
                "content": "Hoa ğŸŒ¸",
                "children": [
                    { "content": "Hoa há»“ng - biá»ƒu tÆ°á»£ng cá»§a tÃ¬nh yÃªu" },
                    { "content": "Hoa hÆ°á»›ng dÆ°Æ¡ng - luÃ´n hÆ°á»›ng vá» máº·t trá»i" }
                ]
                },
                {
                "content": "Rau cá»§ ğŸ¥•",
                "children": [
                    { "content": "CÃ  rá»‘t - tá»‘t cho máº¯t" },
                    { "content": "Báº¯p cáº£i - nhiá»u vitamin" }
                ]
                }
            ]
            },
            {
            "content": "ThiÃªn nhiÃªn ğŸï¸",
            "children": [
                {
                "content": "NÃºi non hÃ¹ng vÄ©",
                "children": [
                    { "content": "Äá»‰nh nÃºi cao vÃºt" },
                    { "content": "Thung lÅ©ng xanh mÆ°á»›t" }
                ]
                },
                {
                "content": "Biá»ƒn cáº£ bao la ğŸŒŠ",
                "children": [
                    { "content": "SÃ³ng vá»— rÃ¬ rÃ o" },
                    { "content": "Sinh váº­t biá»ƒn Ä‘a dáº¡ng" }
                ]
                },
                {
                "content": "Thá»i tiáº¿t â˜€ï¸ğŸŒ§ï¸",
                "children": [
                    { "content": "Trá»i náº¯ng - áº¥m Ã¡p" },
                    { "content": "Trá»i mÆ°a - mÃ¡t máº»" },
                    { "content": "Trá»i giÃ³ - thá»•i máº¡nh" }
                ]
                }
            ]
            }
        ]
    }
        ```
        """

        mock_usage = TokenUsage(
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            model="mock",
            provider="mock",
        )
        self.last_token_usage = mock_usage

        return mock_mindmap, mock_usage

    # ============ RAG ============
    def make_outline_with_rag(self, request: OutlineGenerateRequest):
        """Generate outline using LLM and save result.
        Args:
            request (OutlineGenerateRequest): Request object containing parameters for outline generation.
        Returns:
            Dict: A dictionary containing the generated outline.
        """
        sys_msg = self._system(
            "outline.system.rag",
            None,
        )

        usr_msg = self._system(
            "outline.user",
            request.to_dict(),
        )

        # Build filters for document search
        # Note: Use subject_code (e.g., 'TV', 'T', 'TA') instead of subject name
        filters = {}
        if request.subject:
            filters["subject_code"] = request.subject
        if request.grade:
            # Convert grade to integer if it's numeric (metadata stores it as int)
            try:
                filters["grade"] = int(request.grade)
            except (ValueError, TypeError):
                filters["grade"] = request.grade

        print(f"[DEBUG] RAG filters being applied: {filters}")
        print(
            f"[DEBUG] Request - subject: {request.subject}, grade: {request.grade} (type: {type(request.grade).__name__})"
        )

        result, token_usage = self.llm_executor.rag_batch(
            provider=request.provider,
            model=request.model,
            query=usr_msg,
            system_prompt=sys_msg,
            return_source_documents=True,
            filters=filters if filters else None,
            custom_prompt=None,
            verbose=False,
        )

        # Store token usage for later access
        self.last_token_usage = token_usage
        return result

    def make_presentation_with_rag(self, request: PresentationGenerateRequest):
        sys_msg = self._system(
            "presentation.system.rag",
            None,
        )

        usr_msg = self._system(
            "presentation.user",
            request.to_dict(),
        )

        filters = {}
        if request.subject:
            filters["subject_code"] = request.subject
        if request.grade:
            try:
                filters["grade"] = int(request.grade)
            except (ValueError, TypeError):
                filters["grade"] = request.grade

        print(f"[DEBUG] RAG filters being applied: {filters}")
        print(
            f"[DEBUG] Request - subject: {request.subject}, grade: {request.grade} (type: {type(request.grade).__name__})"
        )

        result, token_usage = self.llm_executor.rag_batch(
            provider=request.provider,
            model=request.model,
            query=usr_msg,
            system_prompt=sys_msg,
            return_source_documents=True,
            filters=filters if filters else None,
            custom_prompt=None,
            verbose=False,
        )

        self.last_token_usage = token_usage
        return result

    def generate_mindmap_with_rag(self, request: MindmapGenerateRequest):
        sys_msg = self._system(
            "mindmap.system.rag",
            None,
        )

        usr_msg = self._system(
            "mindmap.user",
            request.to_dict(),
        )

        filters = {}
        if request.subject:
            filters["subject_code"] = request.subject
        if request.grade:
            try:
                filters["grade"] = int(request.grade)
            except (ValueError, TypeError):
                filters["grade"] = request.grade

        print(f"[DEBUG] RAG filters being applied: {filters}")
        print(
            f"[DEBUG] Request - subject: {request.subject}, grade: {request.grade} (type: {type(request.grade).__name__})"
        )

        result, token_usage = self.llm_executor.rag_batch(
            provider=request.provider,
            model=request.model,
            query=usr_msg,
            system_prompt=sys_msg,
            return_source_documents=True,
            filters=filters if filters else None,
            custom_prompt=None,
            verbose=False,
        )

        self.last_token_usage = token_usage
        return result

    # ============ RAG Stream ============
    def make_outline_rag_stream(
        self, request: OutlineGenerateRequest
    ) -> Tuple[List[str], TokenUsage]:
        result = self.make_outline_with_rag(request)
        return [result["answer"]], self.last_token_usage

    def make_presentation_rag_stream(
        self, request: PresentationGenerateRequest
    ) -> Tuple[List[str], TokenUsage]:
        result = self.make_presentation_with_rag(request)
        return [result["answer"]], self.last_token_usage

    def generate_mindmap_rag_stream(
        self, request: MindmapGenerateRequest
    ) -> Tuple[List[str], TokenUsage]:
        result = self.generate_mindmap_with_rag(request)
        return [result["answer"]], self.last_token_usage
