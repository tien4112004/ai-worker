services:
  local-ai:
    container_name: local-ai
    image: localai/localai:latest
    ports:
      - "8083:8080"
    environment:
      THREADS: "${LOCALAI_THREADS:-8}"
      CONTEXT_SIZE: "${LOCALAI_CONTEXT_SIZE:-2048}"
      WATCHDOG_ENABLED: true
      WATCHDOG_IDLE_ENABLED: true
      WATCHDOG_BUSY_ENABLED: false
      WATCHDOG_IDLE_TIMEOUT: "15m"
      WATCHDOG_BUSY_TIMEOUT: "5m"
      SINGLE_BACKEND: false
      PARALLEL_BACKEND_REQUESTS: true
      CORS: true
      CORS_ALLOW_ORIGINS: "*"
      LOG_LEVEL: "debug"
      PRELOAD_MODELS: ""
    volumes:
      - localai_config:/home/localai/.local-ai/configuration
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 6G
        reservations:
          cpus: "2"
          memory: 3G
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  ai-worker:
    container_name: ai-worker
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - PYTHON_VERSION=3.11
    image: ai-worker:latest
    ports:
      - "8081:8081"
    environment:
      - APP_NAME=ai-worker
      - DEFAULT_MODEL=gemini-2.5-flash-lite
      - LLM_TEMPERATURE=0.7
      - LLM_MAX_TOKENS=2048
      - MAX_RETRIES=3
      - ALLOWED_ORIGINS=http://localhost:8080
      - ALLOWED_CREDENTIALS=true
      - ALLOWED_METHODS=*
      - ALLOWED_HEADERS=*
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=info
    env_file:
      - .env
    volumes:
      # Mount app code for development (remove in production)
      - ./app:/app/app:ro
      # Mount prompts directory
      - ./app/prompts:/app/app/prompts:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - datn-be_default
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 1G
        reservations:
          cpus: "1"
          memory: 256M

  # Optional: Redis for caching (if needed in the future)
  # redis:
  #   image: redis:7-alpine
  #   container_name: ai-worker-redis
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 3s
  #     retries: 3
  #   networks:
  #     - ai-worker-network

volumes:
  localai_config:
  localai_models:
  redis_data:
    driver: local

networks:
  default:
    external: true
    name: datn-be_default
