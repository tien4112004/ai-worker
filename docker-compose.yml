services:
  local-ai:
    container_name: local-ai
    image: localai/localai:latest
    ports:
      - "8083:8080"
    environment:
      THREADS: "${LOCALAI_THREADS:-8}"
      CONTEXT_SIZE: "${LOCALAI_CONTEXT_SIZE:-2048}"
      WATCHDOG_ENABLED: true
      WATCHDOG_IDLE_ENABLED: true
      WATCHDOG_BUSY_ENABLED: false
      WATCHDOG_IDLE_TIMEOUT: "15m"
      WATCHDOG_BUSY_TIMEOUT: "5m"
      SINGLE_BACKEND: false
      PARALLEL_BACKEND_REQUESTS: true
      CORS: true
      CORS_ALLOW_ORIGINS: "*"
      LOG_LEVEL: "debug"
      PRELOAD_MODELS: ""
    volumes:
      - localai_config:/home/localai/.local-ai/configuration
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 6G
        reservations:
          cpus: "2"
          memory: 3G
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - datn-be_default

  ai-worker:
    container_name: ai-worker
    image: ai-worker:latest
    ports:
      - "8081:8081"
    volumes:
      # Mount service account key file
      - ./service-account.json:/app/secrets/gcp-service-account.json:ro
    environment:
      - APP_NAME=ai-worker
      - DEFAULT_MODEL=${DEFAULT_MODEL:-gemini-2.5-flash-lite}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-2048}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-*}
      - ALLOWED_CREDENTIALS=${ALLOWED_CREDENTIALS:-true}
      - ALLOWED_METHODS=${ALLOWED_METHODS:-*}
      - ALLOWED_HEADERS=${ALLOWED_HEADERS:-*}
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=${LOG_LEVEL:-info}
      # Google Cloud Service Account
      - GOOGLE_APPLICATION_CREDENTIALS=/app/secrets/gcp-service-account.json
      # API Keys
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - VERTEX_PROJECT_ID=${VERTEX_PROJECT_ID:-}
      - VERTEX_LOCATION=${VERTEX_LOCATION:-}
      - PHOENIX_SECRET=${PHOENIX_SECRET}
      - PHOENIX_SQL_DATABASE_URL=${PHOENIX_SQL_DATABASE_URL}
      - PHOENIX_PROJECT_NAME=${PHOENIX_PROJECT_NAME}
      - PHOENIX_COLLECTOR_ENDPOINT=${PHOENIX_COLLECTOR_ENDPOINT}
      - PHOENIX_API_KEY=${PHOENIX_API_KEY}

    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 768M
        reservations:
          cpus: "1"
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - datn-be_default

  phoenix:
    image: arizephoenix/phoenix:latest
    container_name: phoenix
    ports:
      - "6006:6006"      # Phoenix UI
      - "4317:4317"      # gRPC collector (OpenTelemetry)
      - "9090:9090"      # HTTP collector (alternative)
    environment:
      # Phoenix configuration
      - PHOENIX_WORKING_DIR=/phoenix/data
      - PHOENIX_PORT=6006
      - PHOENIX_GRPC_PORT=4317

      # Enable authentication
      - PHOENIX_ENABLE_AUTH=true
      - PHOENIX_SECRET=${PHOENIX_SECRET}

      # PostgreSQL
      - PHOENIX_SQL_DATABASE_URL=postgresql://postgres:postgres@postgres:5432/ai-worker-tracesdb
    networks:
      - datn-be_default
    restart: unless-stopped

volumes:
  localai_config:
  localai_models:
  redis_data:
    driver: local
  postgres_data:
    driver: local

networks:
  datn-be_default:
    external: true
